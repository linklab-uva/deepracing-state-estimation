{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import transform_data_for_rnn\n",
    "from data_scraper import scrape_udp_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pickle file already exists\n"
     ]
    }
   ],
   "source": [
    "class StateEstimationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, motion_dir, telemetry_dir, udp_filename, buffer_size=5):\n",
    "        scrape_udp_data(motion_dir, telemetry_dir, filename=udp_filename)\n",
    "        self.X_data, self.y_data = transform_data_for_rnn(udp_filename, buffer_size)\n",
    "        self.X_data = torch.from_numpy(self.X_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "    def __len__(self):\n",
    "        return(self.X_data.shape[0])\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        return x, y\n",
    "\n",
    "udp_dataset = StateEstimationDataset(motion_dir=r\"C:\\\\Users\\\\chros\\\\Documents\\\\Deep Racing\\\\australia_run1\\\\udp_data\\\\motion_packets\", telemetry_dir=r\"C:\\\\Users\\\\chros\\\\Documents\\\\Deep Racing\\\\australia_run1\\\\udp_data\\\\car_telemetry packets\", udp_filename=\"udp_data.pkl\", buffer_size=5)\n",
    "split_id = int(len(udp_dataset)* 0.8)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(udp_dataset, [split_id, (len(udp_dataset) - split_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEstimationNet(nn.Module):\n",
    "    def __init__(self, buffer_size, input_shape, hidden_dim, n_layers, batch_size, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.output_size = 3 * buffer_size\n",
    "        self.input_shape = input_shape\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_shape, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.float()\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(self.batch_size, -1)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StateEstimationNet(buffer_size=5, input_shape=udp_dataset.X_data.shape[-1], hidden_dim=100, n_layers=2, batch_size=batch_size)\n",
    "model.to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "StateEstimationNet(\n  (lstm): LSTM(75, 100, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=100, out_features=15, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "oss: 31992.130859... Val Loss: 29070.511518\n",
      "Epoch: 45/100... Step: 196000... Loss: 66899.242188... Val Loss: 26346.742831\n",
      "Epoch: 46/100... Step: 197000... Loss: 57689.148438... Val Loss: 30229.158145\n",
      "Epoch: 46/100... Step: 198000... Loss: 56776.218750... Val Loss: 28543.165963\n",
      "Epoch: 46/100... Step: 199000... Loss: 63354.886719... Val Loss: 27445.143461\n",
      "Epoch: 46/100... Step: 200000... Loss: 41640.945312... Val Loss: 28657.009247\n",
      "Epoch: 47/100... Step: 201000... Loss: 46379.796875... Val Loss: 34117.882486\n",
      "Epoch: 47/100... Step: 202000... Loss: 96442.929688... Val Loss: 30333.021570\n",
      "Epoch: 47/100... Step: 203000... Loss: 69316.414062... Val Loss: 29618.582342\n",
      "Epoch: 47/100... Step: 204000... Loss: 53366.929688... Val Loss: 29160.152422\n",
      "Epoch: 48/100... Step: 205000... Loss: 66096.453125... Val Loss: 30866.020506\n",
      "Epoch: 48/100... Step: 206000... Loss: 42556.925781... Val Loss: 26267.502875\n",
      "Epoch: 48/100... Step: 207000... Loss: 34911.402344... Val Loss: 24280.947449\n",
      "Epoch: 48/100... Step: 208000... Loss: 53705.128906... Val Loss: 25003.278154\n",
      "Epoch: 48/100... Step: 209000... Loss: 45798.437500... Val Loss: 22172.920092\n",
      "Validation loss decreased (23296.556453 --> 22172.920092).  Saving model ...\n",
      "Epoch: 49/100... Step: 210000... Loss: 70326.359375... Val Loss: 29501.627360\n",
      "Epoch: 49/100... Step: 211000... Loss: 46321.843750... Val Loss: 29134.005739\n",
      "Epoch: 49/100... Step: 212000... Loss: 65339.464844... Val Loss: 31743.700142\n",
      "Epoch: 49/100... Step: 213000... Loss: 76913.218750... Val Loss: 33729.296843\n",
      "Epoch: 50/100... Step: 214000... Loss: 46248.097656... Val Loss: 31473.474081\n",
      "Epoch: 50/100... Step: 215000... Loss: 30840.849609... Val Loss: 28552.871044\n",
      "Epoch: 50/100... Step: 216000... Loss: 53743.511719... Val Loss: 27080.089939\n",
      "Epoch: 50/100... Step: 217000... Loss: 27089.330078... Val Loss: 28380.140433\n",
      "Epoch: 51/100... Step: 218000... Loss: 41526.167969... Val Loss: 29403.138307\n",
      "Epoch: 51/100... Step: 219000... Loss: 40199.035156... Val Loss: 27327.709041\n",
      "Epoch: 51/100... Step: 220000... Loss: 30399.771484... Val Loss: 24021.870305\n",
      "Epoch: 51/100... Step: 221000... Loss: 62278.351562... Val Loss: 27290.167773\n",
      "Epoch: 51/100... Step: 222000... Loss: 62837.429688... Val Loss: 32964.067715\n",
      "Epoch: 52/100... Step: 223000... Loss: 37829.476562... Val Loss: 24729.715572\n",
      "Epoch: 52/100... Step: 224000... Loss: 23279.130859... Val Loss: 24965.093808\n",
      "Epoch: 52/100... Step: 225000... Loss: 48289.832031... Val Loss: 24215.582802\n",
      "Epoch: 52/100... Step: 226000... Loss: 53543.601562... Val Loss: 25811.479690\n",
      "Epoch: 53/100... Step: 227000... Loss: 91806.554688... Val Loss: 30845.768961\n",
      "Epoch: 53/100... Step: 228000... Loss: 47498.878906... Val Loss: 29002.324612\n",
      "Epoch: 53/100... Step: 229000... Loss: 53455.269531... Val Loss: 31777.635841\n",
      "Epoch: 53/100... Step: 230000... Loss: 38169.039062... Val Loss: 31106.985135\n",
      "Epoch: 54/100... Step: 231000... Loss: 53485.976562... Val Loss: 31141.047127\n",
      "Epoch: 54/100... Step: 232000... Loss: 46406.394531... Val Loss: 26719.902882\n",
      "Epoch: 54/100... Step: 233000... Loss: 41931.601562... Val Loss: 27208.897430\n",
      "Epoch: 54/100... Step: 234000... Loss: 46020.175781... Val Loss: 30046.041584\n",
      "Epoch: 54/100... Step: 235000... Loss: 68272.625000... Val Loss: 25853.358198\n",
      "Epoch: 55/100... Step: 236000... Loss: 58244.585938... Val Loss: 24801.278833\n",
      "Epoch: 55/100... Step: 237000... Loss: 50503.660156... Val Loss: 30040.207555\n",
      "Epoch: 55/100... Step: 238000... Loss: 62814.796875... Val Loss: 32817.490465\n",
      "Epoch: 55/100... Step: 239000... Loss: 67927.367188... Val Loss: 28808.035798\n",
      "Epoch: 56/100... Step: 240000... Loss: 48006.691406... Val Loss: 26990.727573\n",
      "Epoch: 56/100... Step: 241000... Loss: 40762.488281... Val Loss: 35506.852741\n",
      "Epoch: 56/100... Step: 242000... Loss: 62224.675781... Val Loss: 29445.559631\n",
      "Epoch: 56/100... Step: 243000... Loss: 56666.710938... Val Loss: 30968.162224\n",
      "Epoch: 57/100... Step: 244000... Loss: 73808.500000... Val Loss: 26916.370149\n",
      "Epoch: 57/100... Step: 245000... Loss: 45746.894531... Val Loss: 26064.463944\n",
      "Epoch: 57/100... Step: 246000... Loss: 40408.449219... Val Loss: 25940.619273\n",
      "Epoch: 57/100... Step: 247000... Loss: 42846.031250... Val Loss: 30492.484399\n",
      "Epoch: 57/100... Step: 248000... Loss: 74100.085938... Val Loss: 25962.869781\n",
      "Epoch: 58/100... Step: 249000... Loss: 41331.945312... Val Loss: 25409.660608\n",
      "Epoch: 58/100... Step: 250000... Loss: 57185.964844... Val Loss: 28248.611023\n",
      "Epoch: 58/100... Step: 251000... Loss: 70983.335938... Val Loss: 29656.419674\n",
      "Epoch: 58/100... Step: 252000... Loss: 39947.117188... Val Loss: 24159.107618\n",
      "Epoch: 59/100... Step: 253000... Loss: 35676.359375... Val Loss: 22000.928688\n",
      "Validation loss decreased (22172.920092 --> 22000.928688).  Saving model ...\n",
      "Epoch: 59/100... Step: 254000... Loss: 78648.523438... Val Loss: 26861.371006\n",
      "Epoch: 59/100... Step: 255000... Loss: 55429.242188... Val Loss: 25895.569723\n",
      "Epoch: 59/100... Step: 256000... Loss: 56877.156250... Val Loss: 26913.308942\n",
      "Epoch: 59/100... Step: 257000... Loss: 42350.210938... Val Loss: 24084.168317\n",
      "Epoch: 60/100... Step: 258000... Loss: 43027.437500... Val Loss: 24881.238281\n",
      "Epoch: 60/100... Step: 259000... Loss: 65710.234375... Val Loss: 24997.788642\n",
      "Epoch: 60/100... Step: 260000... Loss: 49833.187500... Val Loss: 21275.298776\n",
      "Validation loss decreased (22000.928688 --> 21275.298776).  Saving model ...\n",
      "Epoch: 60/100... Step: 261000... Loss: 72492.718750... Val Loss: 33191.024655\n",
      "Epoch: 61/100... Step: 262000... Loss: 66689.710938... Val Loss: 31421.933735\n",
      "Epoch: 61/100... Step: 263000... Loss: 24313.314453... Val Loss: 24473.052619\n",
      "Epoch: 61/100... Step: 264000... Loss: 33036.136719... Val Loss: 26224.484676\n",
      "Epoch: 61/100... Step: 265000... Loss: 54097.515625... Val Loss: 31424.430630\n",
      "Epoch: 62/100... Step: 266000... Loss: 36946.308594... Val Loss: 26522.283541\n",
      "Epoch: 62/100... Step: 267000... Loss: 47550.046875... Val Loss: 24323.033929\n",
      "Epoch: 62/100... Step: 268000... Loss: 39428.578125... Val Loss: 21304.124255\n",
      "Epoch: 62/100... Step: 269000... Loss: 44145.343750... Val Loss: 20965.436395\n",
      "Validation loss decreased (21275.298776 --> 20965.436395).  Saving model ...\n",
      "Epoch: 62/100... Step: 270000... Loss: 52940.152344... Val Loss: 25535.875336\n",
      "Epoch: 63/100... Step: 271000... Loss: 47947.578125... Val Loss: 28581.388164\n",
      "Epoch: 63/100... Step: 272000... Loss: 51639.675781... Val Loss: 25318.454215\n",
      "Epoch: 63/100... Step: 273000... Loss: 47520.988281... Val Loss: 26103.342563\n",
      "Epoch: 63/100... Step: 274000... Loss: 75616.718750... Val Loss: 23373.420703\n",
      "Epoch: 64/100... Step: 275000... Loss: 28193.638672... Val Loss: 25414.380909\n",
      "Epoch: 64/100... Step: 276000... Loss: 39120.675781... Val Loss: 25687.124891\n",
      "Epoch: 64/100... Step: 277000... Loss: 48636.871094... Val Loss: 26449.479987\n",
      "Epoch: 64/100... Step: 278000... Loss: 32369.537109... Val Loss: 24022.659482\n",
      "Epoch: 65/100... Step: 279000... Loss: 52862.394531... Val Loss: 27146.615660\n",
      "Epoch: 65/100... Step: 280000... Loss: 33933.562500... Val Loss: 24718.331187\n",
      "Epoch: 65/100... Step: 281000... Loss: 41753.253906... Val Loss: 24374.448372\n",
      "Epoch: 65/100... Step: 282000... Loss: 41583.480469... Val Loss: 22276.511744\n",
      "Epoch: 65/100... Step: 283000... Loss: 30903.183594... Val Loss: 25147.934907\n",
      "Epoch: 66/100... Step: 284000... Loss: 49444.937500... Val Loss: 24430.821376\n",
      "Epoch: 66/100... Step: 285000... Loss: 18961.259766... Val Loss: 24900.258634\n",
      "Epoch: 66/100... Step: 286000... Loss: 27489.068359... Val Loss: 32959.782213\n",
      "Epoch: 66/100... Step: 287000... Loss: 42016.203125... Val Loss: 29353.525280\n",
      "Epoch: 67/100... Step: 288000... Loss: 34844.679688... Val Loss: 27112.821756\n",
      "Epoch: 67/100... Step: 289000... Loss: 45633.253906... Val Loss: 29891.651209\n",
      "Epoch: 67/100... Step: 290000... Loss: 40009.429688... Val Loss: 24775.400979\n",
      "Epoch: 67/100... Step: 291000... Loss: 26879.427734... Val Loss: 19637.813116\n",
      "Validation loss decreased (20965.436395 --> 19637.813116).  Saving model ...\n",
      "Epoch: 68/100... Step: 292000... Loss: 59952.449219... Val Loss: 19664.729687\n",
      "Epoch: 68/100... Step: 293000... Loss: 25533.376953... Val Loss: 19017.817917\n",
      "Validation loss decreased (19637.813116 --> 19017.817917).  Saving model ...\n",
      "Epoch: 68/100... Step: 294000... Loss: 44313.242188... Val Loss: 21048.435754\n",
      "Epoch: 68/100... Step: 295000... Loss: 19939.441406... Val Loss: 19074.252323\n",
      "Epoch: 68/100... Step: 296000... Loss: 39033.175781... Val Loss: 21257.670313\n",
      "Epoch: 69/100... Step: 297000... Loss: 48541.636719... Val Loss: 18635.168825\n",
      "Validation loss decreased (19017.817917 --> 18635.168825).  Saving model ...\n",
      "Epoch: 69/100... Step: 298000... Loss: 22573.326172... Val Loss: 17151.082979\n",
      "Validation loss decreased (18635.168825 --> 17151.082979).  Saving model ...\n",
      "Epoch: 69/100... Step: 299000... Loss: 34989.507812... Val Loss: 18732.879738\n",
      "Epoch: 69/100... Step: 300000... Loss: 28313.863281... Val Loss: 18135.004738\n",
      "Epoch: 70/100... Step: 301000... Loss: 43462.687500... Val Loss: 17550.990008\n",
      "Epoch: 70/100... Step: 302000... Loss: 52597.093750... Val Loss: 19369.340855\n",
      "Epoch: 70/100... Step: 303000... Loss: 39970.324219... Val Loss: 20451.820030\n",
      "Epoch: 70/100... Step: 304000... Loss: 23343.054688... Val Loss: 22475.366325\n",
      "Epoch: 71/100... Step: 305000... Loss: 36435.375000... Val Loss: 20003.505388\n",
      "Epoch: 71/100... Step: 306000... Loss: 56645.781250... Val Loss: 19147.319844\n",
      "Epoch: 71/100... Step: 307000... Loss: 47380.468750... Val Loss: 18962.795316\n",
      "Epoch: 71/100... Step: 308000... Loss: 37045.351562... Val Loss: 18724.887318\n",
      "Epoch: 71/100... Step: 309000... Loss: 78862.187500... Val Loss: 22682.161371\n",
      "Epoch: 72/100... Step: 310000... Loss: 42476.378906... Val Loss: 16613.163064\n",
      "Validation loss decreased (17151.082979 --> 16613.163064).  Saving model ...\n",
      "Epoch: 72/100... Step: 311000... Loss: 43188.277344... Val Loss: 18229.432527\n",
      "Epoch: 72/100... Step: 312000... Loss: 25942.865234... Val Loss: 17030.319287\n",
      "Epoch: 72/100... Step: 313000... Loss: 27576.142578... Val Loss: 17534.421205\n",
      "Epoch: 73/100... Step: 314000... Loss: 30477.250000... Val Loss: 17938.903864\n",
      "Epoch: 73/100... Step: 315000... Loss: 54018.429688... Val Loss: 26354.387612\n",
      "Epoch: 73/100... Step: 316000... Loss: 30643.251953... Val Loss: 17357.720430\n",
      "Epoch: 73/100... Step: 317000... Loss: 51076.425781... Val Loss: 19421.938541\n",
      "Epoch: 73/100... Step: 318000... Loss: 24089.667969... Val Loss: 17418.535776\n",
      "Epoch: 74/100... Step: 319000... Loss: 47794.410156... Val Loss: 19265.103447\n",
      "Epoch: 74/100... Step: 320000... Loss: 33184.722656... Val Loss: 16021.189208\n",
      "Validation loss decreased (16613.163064 --> 16021.189208).  Saving model ...\n",
      "Epoch: 74/100... Step: 321000... Loss: 54263.296875... Val Loss: 17770.786855\n",
      "Epoch: 74/100... Step: 322000... Loss: 25653.398438... Val Loss: 16842.794692\n",
      "Epoch: 75/100... Step: 323000... Loss: 27692.669922... Val Loss: 20170.404066\n",
      "Epoch: 75/100... Step: 324000... Loss: 40156.890625... Val Loss: 17373.442114\n",
      "Epoch: 75/100... Step: 325000... Loss: 27192.810547... Val Loss: 18059.574415\n",
      "Epoch: 75/100... Step: 326000... Loss: 70363.617188... Val Loss: 19543.763003\n",
      "Epoch: 76/100... Step: 327000... Loss: 34336.632812... Val Loss: 15355.834381\n",
      "Validation loss decreased (16021.189208 --> 15355.834381).  Saving model ...\n",
      "Epoch: 76/100... Step: 328000... Loss: 34815.812500... Val Loss: 17357.121301\n",
      "Epoch: 76/100... Step: 329000... Loss: 28695.728516... Val Loss: 16383.192508\n",
      "Epoch: 76/100... Step: 330000... Loss: 35113.265625... Val Loss: 17206.043500\n",
      "Epoch: 76/100... Step: 331000... Loss: 33583.015625... Val Loss: 21138.127001\n",
      "Epoch: 77/100... Step: 332000... Loss: 42291.203125... Val Loss: 21988.704047\n",
      "Epoch: 77/100... Step: 333000... Loss: 28174.667969... Val Loss: 24337.181836\n",
      "Epoch: 77/100... Step: 334000... Loss: 30565.457031... Val Loss: 20168.567419\n",
      "Epoch: 77/100... Step: 335000... Loss: 48379.593750... Val Loss: 19768.565170\n",
      "Epoch: 78/100... Step: 336000... Loss: 27858.521484... Val Loss: 18721.775235\n",
      "Epoch: 78/100... Step: 337000... Loss: 35976.910156... Val Loss: 20134.195330\n",
      "Epoch: 78/100... Step: 338000... Loss: 37373.027344... Val Loss: 19852.925570\n",
      "Epoch: 78/100... Step: 339000... Loss: 26932.017578... Val Loss: 17087.064171\n",
      "Epoch: 79/100... Step: 340000... Loss: 30703.167969... Val Loss: 16679.748618\n",
      "Epoch: 79/100... Step: 341000... Loss: 27923.068359... Val Loss: 19482.858106\n",
      "Epoch: 79/100... Step: 342000... Loss: 34855.925781... Val Loss: 15182.226221\n",
      "Validation loss decreased (15355.834381 --> 15182.226221).  Saving model ...\n",
      "Epoch: 79/100... Step: 343000... Loss: 50374.546875... Val Loss: 13963.397732\n",
      "Validation loss decreased (15182.226221 --> 13963.397732).  Saving model ...\n",
      "Epoch: 79/100... Step: 344000... Loss: 38275.449219... Val Loss: 15180.650558\n",
      "Epoch: 80/100... Step: 345000... Loss: 27458.082031... Val Loss: 16502.741252\n",
      "Epoch: 80/100... Step: 346000... Loss: 45610.992188... Val Loss: 19078.893666\n",
      "Epoch: 80/100... Step: 347000... Loss: 50616.027344... Val Loss: 19154.212722\n",
      "Epoch: 80/100... Step: 348000... Loss: 43941.800781... Val Loss: 15652.312969\n",
      "Epoch: 81/100... Step: 349000... Loss: 18508.216797... Val Loss: 18943.288036\n",
      "Epoch: 81/100... Step: 350000... Loss: 46364.222656... Val Loss: 26012.826122\n",
      "Epoch: 81/100... Step: 351000... Loss: 28777.626953... Val Loss: 23886.630167\n",
      "Epoch: 81/100... Step: 352000... Loss: 43679.695312... Val Loss: 27067.337036\n",
      "Epoch: 82/100... Step: 353000... Loss: 75817.890625... Val Loss: 26494.327546\n",
      "Epoch: 82/100... Step: 354000... Loss: 55010.828125... Val Loss: 22288.081513\n",
      "Epoch: 82/100... Step: 355000... Loss: 42297.472656... Val Loss: 19351.686067\n",
      "Epoch: 82/100... Step: 356000... Loss: 40818.367188... Val Loss: 17158.826662\n",
      "Epoch: 82/100... Step: 357000... Loss: 31953.197266... Val Loss: 16837.046794\n",
      "Epoch: 83/100... Step: 358000... Loss: 35478.964844... Val Loss: 14789.380055\n",
      "Epoch: 83/100... Step: 359000... Loss: 31827.050781... Val Loss: 16111.664689\n",
      "Epoch: 83/100... Step: 360000... Loss: 78076.968750... Val Loss: 35640.563385\n",
      "Epoch: 83/100... Step: 361000... Loss: 30943.306641... Val Loss: 19787.000743\n",
      "Epoch: 84/100... Step: 362000... Loss: 43339.550781... Val Loss: 20377.493959\n",
      "Epoch: 84/100... Step: 363000... Loss: 72031.593750... Val Loss: 20845.690203\n",
      "Epoch: 84/100... Step: 364000... Loss: 43018.945312... Val Loss: 20041.593881\n",
      "Epoch: 84/100... Step: 365000... Loss: 57707.238281... Val Loss: 25131.298581\n",
      "Epoch: 85/100... Step: 366000... Loss: 53956.132812... Val Loss: 18756.162627\n",
      "Epoch: 85/100... Step: 367000... Loss: 40712.753906... Val Loss: 25468.048434\n",
      "Epoch: 85/100... Step: 368000... Loss: 43636.144531... Val Loss: 16887.765537\n",
      "Epoch: 85/100... Step: 369000... Loss: 30640.101562... Val Loss: 19091.326545\n",
      "Epoch: 85/100... Step: 370000... Loss: 44899.023438... Val Loss: 21474.766714\n",
      "Epoch: 86/100... Step: 371000... Loss: 40235.902344... Val Loss: 19422.209046\n",
      "Epoch: 86/100... Step: 372000... Loss: 44362.859375... Val Loss: 20649.320434\n",
      "Epoch: 86/100... Step: 373000... Loss: 51435.902344... Val Loss: 22638.424900\n",
      "Epoch: 86/100... Step: 374000... Loss: 19950.611328... Val Loss: 23899.949360\n",
      "Epoch: 87/100... Step: 375000... Loss: 42101.078125... Val Loss: 18493.790306\n",
      "Epoch: 87/100... Step: 376000... Loss: 39478.191406... Val Loss: 15812.077882\n",
      "Epoch: 87/100... Step: 377000... Loss: 34984.019531... Val Loss: 15579.449458\n",
      "Epoch: 87/100... Step: 378000... Loss: 50617.746094... Val Loss: 15701.041616\n",
      "Epoch: 87/100... Step: 379000... Loss: 48394.871094... Val Loss: 14599.472480\n",
      "Epoch: 88/100... Step: 380000... Loss: 29745.251953... Val Loss: 14485.573226\n",
      "Epoch: 88/100... Step: 381000... Loss: 30138.947266... Val Loss: 14445.767116\n",
      "Epoch: 88/100... Step: 382000... Loss: 43293.468750... Val Loss: 19352.491209\n",
      "Epoch: 88/100... Step: 383000... Loss: 33499.210938... Val Loss: 14921.188224\n",
      "Epoch: 89/100... Step: 384000... Loss: 44197.292969... Val Loss: 14187.937687\n",
      "Epoch: 89/100... Step: 385000... Loss: 34045.308594... Val Loss: 19813.604070\n",
      "Epoch: 89/100... Step: 386000... Loss: 56049.203125... Val Loss: 14471.975928\n",
      "Epoch: 89/100... Step: 387000... Loss: 53441.371094... Val Loss: 15263.399305\n",
      "Epoch: 90/100... Step: 388000... Loss: 25642.712891... Val Loss: 14209.020487\n",
      "Epoch: 90/100... Step: 389000... Loss: 24949.757812... Val Loss: 16992.797005\n",
      "Epoch: 90/100... Step: 390000... Loss: 19523.785156... Val Loss: 12732.538371\n",
      "Validation loss decreased (13963.397732 --> 12732.538371).  Saving model ...\n",
      "Epoch: 90/100... Step: 391000... Loss: 26833.101562... Val Loss: 13528.354179\n",
      "Epoch: 90/100... Step: 392000... Loss: 63104.386719... Val Loss: 12508.054805\n",
      "Validation loss decreased (12732.538371 --> 12508.054805).  Saving model ...\n",
      "Epoch: 91/100... Step: 393000... Loss: 20752.046875... Val Loss: 11938.156289\n",
      "Validation loss decreased (12508.054805 --> 11938.156289).  Saving model ...\n",
      "Epoch: 91/100... Step: 394000... Loss: 36542.976562... Val Loss: 12927.612562\n",
      "Epoch: 91/100... Step: 395000... Loss: 43306.144531... Val Loss: 12962.526323\n",
      "Epoch: 91/100... Step: 396000... Loss: 30807.896484... Val Loss: 13717.156011\n",
      "Epoch: 92/100... Step: 397000... Loss: 34288.523438... Val Loss: 13599.091504\n",
      "Epoch: 92/100... Step: 398000... Loss: 25118.847656... Val Loss: 15401.126145\n",
      "Epoch: 92/100... Step: 399000... Loss: 31349.886719... Val Loss: 15285.291168\n",
      "Epoch: 92/100... Step: 400000... Loss: 65563.445312... Val Loss: 19532.167136\n",
      "Epoch: 93/100... Step: 401000... Loss: 41830.714844... Val Loss: 15410.858735\n",
      "Epoch: 93/100... Step: 402000... Loss: 26693.111328... Val Loss: 14163.493914\n",
      "Epoch: 93/100... Step: 403000... Loss: 48365.250000... Val Loss: 24381.703277\n",
      "Epoch: 93/100... Step: 404000... Loss: 42676.675781... Val Loss: 19214.089046\n",
      "Epoch: 93/100... Step: 405000... Loss: 67082.359375... Val Loss: 22803.403665\n",
      "Epoch: 94/100... Step: 406000... Loss: 46069.320312... Val Loss: 19915.370583\n",
      "Epoch: 94/100... Step: 407000... Loss: 62007.386719... Val Loss: 22845.815271\n",
      "Epoch: 94/100... Step: 408000... Loss: 49591.109375... Val Loss: 22301.797583\n",
      "Epoch: 94/100... Step: 409000... Loss: 36198.539062... Val Loss: 24615.371916\n",
      "Epoch: 95/100... Step: 410000... Loss: 18264.484375... Val Loss: 19706.672496\n",
      "Epoch: 95/100... Step: 411000... Loss: 54787.136719... Val Loss: 20598.476089\n",
      "Epoch: 95/100... Step: 412000... Loss: 51500.109375... Val Loss: 16717.206338\n",
      "Epoch: 95/100... Step: 413000... Loss: 42879.535156... Val Loss: 15028.354425\n",
      "Epoch: 96/100... Step: 414000... Loss: 24768.544922... Val Loss: 14629.996767\n",
      "Epoch: 96/100... Step: 415000... Loss: 30988.320312... Val Loss: 14968.546081\n",
      "Epoch: 96/100... Step: 416000... Loss: 47888.558594... Val Loss: 13777.487569\n",
      "Epoch: 96/100... Step: 417000... Loss: 42956.093750... Val Loss: 15335.878120\n",
      "Epoch: 96/100... Step: 418000... Loss: 26277.722656... Val Loss: 15718.616977\n",
      "Epoch: 97/100... Step: 419000... Loss: 43037.777344... Val Loss: 15093.948354\n",
      "Epoch: 97/100... Step: 420000... Loss: 33137.007812... Val Loss: 18392.814359\n",
      "Epoch: 97/100... Step: 421000... Loss: 41868.542969... Val Loss: 17949.689840\n",
      "Epoch: 97/100... Step: 422000... Loss: 36963.136719... Val Loss: 17637.426142\n",
      "Epoch: 98/100... Step: 423000... Loss: 51295.820312... Val Loss: 17285.965398\n",
      "Epoch: 98/100... Step: 424000... Loss: 38286.308594... Val Loss: 23243.216047\n",
      "Epoch: 98/100... Step: 425000... Loss: 56010.328125... Val Loss: 20373.247238\n",
      "Epoch: 98/100... Step: 426000... Loss: 31951.263672... Val Loss: 18954.842219\n",
      "Epoch: 99/100... Step: 427000... Loss: 22163.574219... Val Loss: 19187.441824\n",
      "Epoch: 99/100... Step: 428000... Loss: 33904.777344... Val Loss: 17958.914517\n",
      "Epoch: 99/100... Step: 429000... Loss: 61788.968750... Val Loss: 24095.053543\n",
      "Epoch: 99/100... Step: 430000... Loss: 70445.789062... Val Loss: 31395.701130\n",
      "Epoch: 99/100... Step: 431000... Loss: 52192.421875... Val Loss: 24513.159536\n",
      "Epoch: 100/100... Step: 432000... Loss: 38058.738281... Val Loss: 16895.006089\n",
      "Epoch: 100/100... Step: 433000... Loss: 36575.390625... Val Loss: 19131.142129\n",
      "Epoch: 100/100... Step: 434000... Loss: 54065.062500... Val Loss: 19995.665206\n",
      "Epoch: 100/100... Step: 435000... Loss: 54940.878906... Val Loss: 23614.923335\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "counter = 0\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden()\n",
    "    for inputs, labels in train_data_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = loss_function(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if counter%1000 == 0:\n",
    "            val_h = model.init_hidden()\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_data_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = loss_function(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "            model.train()\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "    print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "          \"Loss: {:.6f}...\".format(loss.item()),\n",
    "          \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}